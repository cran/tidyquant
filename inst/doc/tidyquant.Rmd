---
title: "Introduction to tidyquant"
author: "Matt Dancho"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Introduction to tidyquant}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      fig.width = 6)
# devtools::load_all() # Travis CI fails on load_all()
```

> Bringing quantitative financial analysis to the tidyverse

# Overview

`tidyquant` integrates the best quantitative resources for collecting and analyzing quantitative data, `xts` and `zoo`, `quantmod` and `TTR`, with the tidy data infrastructure of the `tidyverse` allowing for seamless interaction between each and working within the `tidyverse`. 

The three primary quantitative packages that are the backbone for quantitative financial analysis in _R programming_ are: 

* [xts](https://CRAN.R-project.org/package=xts), or [eXtensible time series](http://joshuaulrich.github.io/xts/index.html): The data structure for handling time-series data. The underlying time-series structure is `zoo`, which is also integrated.
* [quantmod](https://CRAN.R-project.org/package=quantmod), or [Quantitative Financial Modelling & Trading Framework for R](http://www.quantmod.com/): A package designed for retrieving, manipulating, and modeling and  quantitative data.
* [TTR](https://CRAN.R-project.org/package=TTR), or Technical Trading Rules: A package that includes various functions to compute technical trading equations for quantitative or trading data.

The [tidy data principles](https://www.jstatsoft.org/article/view/v059i10) are a cornerstone of data management and data modeling workflow. The foundation for tidy data management is the `tidyverse`, a collection of _R packages_: `ggplot2`, `dplyr`, `tidyr`, `purrr`, `readr`, `tibble`, that work in harmony, are built for scalability, and are well documented in [R for Data Science](http://r4ds.had.co.nz/). Using this infrastructure and the core tidy concepts, we can integrate the tidy data principles with the best quantitative financial analysis packages using the package, `tidyquant`. 

# Prerequisites

Load the `tidyquant` package to get started.

```{r}
# Loads tidyquant, tidyverse, lubridate, xts, quantmod, TTR 
library(tidyquant)  
```


# Benefits

__The `tidyquant` philosophy__:

* __A few core functions with a lot of power, that__
* __leverage the quantitative analysis power of `xts`, `quantmod` and `TTR`, and are__
* __designed to be used and scaled with the `tidyverse`.__

<a class="anchor" id="core-functions"></a>

## A Few Core Functions with A Lot of Power

Minimizing the number of functions reduces the learning curve. Functions are grouped into verbs for efficient collection and manipulation of quantitative data: 

* __Get Quantitative Data, `tq_get()`__: A one-stop shop to get data from various web-sources.  

* __Transform, `tq_transform()`, and Mutate, `tq_mutate()`, Quantitative Data__: These are the workhorse functions that wrap around the `xts`, `quantmod`, and `TTR` packages. 

* __Coerce Quantitative Data Between tibble and xts formats, `as_tibble()` and `as_xts()`__: Coercing `xts`, `zoo`, `timeSeries`, and the other various _R_ time-based objects to and from `tibble` or `data.frame` objects was a pain due to the date/time being stored as row names in time-based objects. The tidyquant `as_tibble()` and `as_xts()` functions enable preservation of row names during coercion.

<a class="anchor" id="tq-get"></a>

### Get Quantitative Data

The `tq_get()` function is used to collect all data by changing the `get` argument. The options include stock lists for 18 stock indexes from marketvolume.com, stock prices, dividends and splits from Yahoo Finance, financial statements from Google Finance, metal prices and exchange rates from Oanda, and economic data from the FRED database. To see the full list, execute `tq_get_options()`.

```{r}
tq_get_options()
```


__Stock Index__:

A wide range of stock index / exchange lists can be retrieved using `get = "stock.index"`. To get a full list of the options, use `tq_get_stock_index_options()`. 

```{r}
tq_get_stock_index_options()
```

Set `x` as one of the options in the list of options above, and `get = "stock.index"` to get the desired stock index / exchange.

```{r}
tq_get("sp500", get = "stock.index")
```

The data source is [www.marketvolume.com](http://www.marketvolume.com/indexes_exchanges/).

__Stock Prices, Dividends and Splits__:

The stock prices can be retrieved succinctly using `get = "stock.prices"`.  

```{r}
aapl_prices  <- tq_get("AAPL", get = "stock.prices", from = " 1990-01-01")
aapl_prices 
```


Dividends are obtained using `get = "dividends"`. 

```{r}
aapl_divs <- tq_get("AAPL", get = "dividends", from = "1990-01-01")
aapl_divs
```

Stock splits are obtained using `get = "splits"`. 

```{r}
aapl_splits <- tq_get("AAPL", get = "splits", from = "1990-01-01")
aapl_splits
```

The data source is [yahoo finance](https://finance.yahoo.com/).

__Financial Statements__:

For any given stock, a total of six financials statements are retrieved as nested tibbles, one for each combination of statement type (Income Statement, Balance Sheet, and Cash Flow) and period (by annual and quarter). 

```{r}
aapl_financials <- tq_get("AAPL", get = "financials")
aapl_financials
```

The statement information can be extracted by selecting (`dplyr::select()`) and filtering (`dplyr::filter()`) to the desired statement and unnesting (`tidyr::unnest()`) the results.

```{r}
aapl_financials %>%
    filter(type == "IS") %>%
    select(annual) %>%
    unnest()
```

A slightly more powerful example is looking at all quarterly statements together. This is easy to do with `unnest` and `spread` from the `tidyr` package.

```{r}
aapl_financials %>%
    unnest(quarter) %>% 
    spread(key = date, value = value)
```


The data source is [google finance](https://www.google.com/finance).


__Key Ratios__:

For any given stock, the key ratios are available for 10 years that are classified into the following sections:

* __Financials__: These ratios include gross margin %, operating margin %, EPS, book value per share, and more.
* __Profitability__: These ratios include margin as a percentage of sales (gross margin, operating margin, EBT margin, etc) and profitability metrics such as tax rate %, asset turnover, ROA, financial leverage, ROE, return on invested capital, and more.
* __Growth__: These ratios include year over year, 3-year average, 5-year average, and 10-year average growth rates for revenue, operating income, net income, and EPS.
* __Cash Flow__: These ratios include operating cash flow growth % YOY, free cash flow growth % YOY, capital expenditure as a % of sales, and more.
* __Financial Health__: These ratios include balance sheet items as a percentage of total assets and liabilities, and liquidty/financial health metrics such as current ratio, quick ratio, debt/equity, and financial leverage.
* __Efficiency Ratios__: These ratios include days sales outstanding, days inventory, inventory turnover, asset turnover and more. 
* __Valuation Ratios__: These ratios include price to earnings (P/E), price to sales (P/S), price to book (P/B), and price to operating cash flow. 

To get the key ratios:

```{r}
aapl_key_ratios <- tq_get("AAPL", get = "key.ratios")
aapl_key_ratios
```

The ratios can be filtered and unnested to peel away the hierarchical nesting layers and access the underlying data:

```{r}
aapl_key_ratios %>%
    filter(section == "Valuation Ratios") %>%
    unnest()
```

Once we have a section, we can quickly visualize the ratios:

```{r}
aapl_key_ratios %>%
    filter(section == "Valuation Ratios") %>%
    unnest() %>%
    ggplot(aes(x = date, y = value)) + 
    geom_line(aes(col = forcats::fct_reorder2(category, date, value))) +
    labs(title = "10-Year Historical Valuation Ratios for AAPL", x = "", 
         y = "", col = "") 
```

The data source is [Morningstar](http://www.morningstar.com/).

<a class="anchor" id="economic-data"></a>

__Economic Data__: 

A wealth of economic data can be extracted from the Federal Reserve Economic Data (FRED) database. The [WTI Crude Oil Prices](https://fred.stlouisfed.org/series/DCOILWTICO) are shown below.

```{r,}
wti_price_usd <- tq_get("DCOILWTICO", get = "economic.data")
wti_price_usd 
```


The FRED contains literally over 10K data sets that are free to use. See the [FRED categories](https://fred.stlouisfed.org/categories) to narrow down the data base and to get data codes. 


__Exchange Rates__:

Exchange rates are entered as currency pairs using "/" notation (e.g `"EUR/USD"`), and by setting `get = "exchange.rates"`. 

```{r}
eur_usd <- tq_get("EUR/USD", get = "exchange.rates", from = "2000-01-01")
eur_usd 
```

The data source is [Oanda](https://www.oanda.com/), and list of currencies to compare can be found on [Oanda's currency converter](https://www.oanda.com/currency/converter/). It may make more sense to get this data from the FRED (See [Economic Data](#economic-data)) since the max period for Oanda is 5-years.

__Metal Prices__:

Metal prices are very similar to stock prices. Set `get = "metal.prices"` along with the appropriate commodity symbol (e.g. XAU (gold) , XAG (silver), XPD (palladium), or XPT (platinum)). 

```{r}
plat_price_eur <- tq_get("plat", get = "metal.prices", 
                         from = "2000-01-01", base.currency = "EUR")
plat_price_eur 
```

The data source is [Oanda](https://www.oanda.com/). It may make more sense to get this data from the FRED (See [Economic Data](#economic-data)) since the max period for Oanda is 5-years.

<a class="anchor" id="tq-transform"></a>

### Transform and Mutate Quantitative Data

Transform and mutate functions enable the `xts`, `quantmod` and `TTR` functions to shine (see [Leverage the Quantitative Power of `xts`, `quantmod` and `TTR`](#quant-power)):

__Transform Quantitative Data, `tq_transform()`__: 

Transforms the results of `tq_get()`. The result is typically a different shape than the input (hence "transformed"), although this is not a requirement. An example is periodicity aggregation from daily to monthly.

```{r}
fb_prices <- tq_get("FB") 
fb_prices %>%
    tq_transform(ohlc_fun = OHLCV, transform_fun = to.monthly)
```

Let's go through what happened. `ohlc_fun` is one of the various quantmod Open, High, Low, Close (OHLC) functions (see `?quantmod::OHLC`). The function returns a column or set of columns from data that are passed to the `transform_fun`. In example above, `OHLCV` selects the full list of prices and volumes from `data`, and sends this to the transform function, `to.monthly`, which transforms the periodicity from daily to monthly. Additional arguments can be passed to the `transform_fun` by way of `...`. 


__Mutate Quantitative Data, `tq_mutate()`__: 

Adds a column or set of columns to the tibble with the calculated attributes (hence the original tibble is returned, mutated with the additional columns). An example is getting the `MACD` from `Cl` (close price), which mutates the original input by adding MACD and Signal columns. 

```{r}
fb_prices %>%
    tq_mutate(ohlc_fun = Cl, mutate_fun = MACD)
```

Note that a mutation can occur if, and only if, the mutation has the same structure of the original tibble. In other words, the calculation must have the same number of rows and row.names (or date fields), otherwise the mutation cannot be performed.

__xy Variants, `tq_transform_xy` and `tq_mutate_xy`__: 

Enables working with:

1. Transformation functions that require two primary inputs (e.g. EVWMA, VWAP, etc) 
2. Data that is not in OHLC format. 

_Transformation with two primary inputs_:

EVWMA (exponential volume-weighted moving average) requires two inputs, price and volume, that are not in OHLC code format. To work with these columns, we can switch to the xy variants, `tq_transform_xy()` and `tq_mutate_xy()`. The only difference is instead of an `ohlc_fun` argument, you use `x` and `y` arguments to pass the columns needed based on the `transform_fun` or `mutate_fun` documentation.

```{r, message=FALSE, warning=FALSE}
fb_prices %>%
    tq_mutate_xy(x = close, y = volume, mutate_fun = EVWMA)
```

_Working with non-OHLC data_:

Returns from FRED, Oanda, and other sources do not have open, high, low, close, and volume (OHLCV) format. The following example shows how to transform WTI Crude daily prices to monthly prices. Since we only have a single column to pass, set the `x = price` and leave the `y = NULL`. This sends the price column to the `to.period` transformation fuction. 

```{r, message=FALSE, warning=FALSE}
wti_prices <- tq_get("DCOILWTICO", get = "economic.data") 
wti_prices %>%    
    tq_transform_xy(x = price, transform_fun = to.period,
                    period = "months")
```

<a class="anchor" id="tq-coerce"></a>

### Coercing Time Series Objects To and From Tibble

Sometimes you want to work using a `tibble` and other times you want to work using a `xts` object. The `as_tibble()` and `as_xts()` functions are the key.

__Coerce from time-series to tibble, `as_tibble()`__:

The `tidyquant::as_tibble()` function includes a `preserve_row_names` argument, which is useful when coercing one of the many time formats (e.g. `xts`, `zoo`, `timeSeries`, `ts`) or `matrix` objects that contain valuable information in the row names. This makes bridging the gap between the various quantitative analysis packages and the `tidyverse` much easier.

Let's start with an `xts` object.

```{r}
# Create xts object from a matrix
vals = matrix(c(500, 504, 503))
date = c("2016-01-01", "2016-01-02", "2016-01-03") 
rownames(vals) <- date
time_series_xts <- as_xts(vals)
time_series_xts
```

We can easily coerce to `tibble` by setting `preserve_row_names = TRUE`. Note the return column is `row.names` with class of `character`.

```{r}
time_series_tbl <- as_tibble(time_series_xts, preserve_row_names = TRUE)
time_series_tbl
```

Converting to date is one extra step with `lubridate`.

```{r}
time_series_tbl <- time_series_tbl %>%
    mutate(row.names = lubridate::ymd(row.names))
time_series_tbl
```

__Coerce from tibble to xts, `as_xts()`__:

We can convert back to `xts` with the tidyquant `as_xts()` function. Make sure to set the date column (`date_col`) argument to the column name containing the date (`date_col = row.names`). The date column must be in a date format (inherits either `Date` or `POSIXct` classes).

```{r}
time_series_xts <- time_series_tbl %>%
    as_xts(date_col = row.names)
time_series_xts
```

<a class="anchor" id="tidyverse"></a>

## Working in the tidyverse

You probably already know and love `tidyverse` packages like `ggplot2`, `dplyr`, `tidyr`, `purrr`, `readr`, and `tibble` along with `lubridate` for working with date and datetime. `tidyquant` works solely in tibbles, so all of the `tidyverse` functionality is intact. 

A simple example inspired by [Kan Nishida's blog](https://blog.exploratory.io/introducing-time-series-analysis-with-dplyr-60683587cf8a#.w6pvyi3d2) shows the `dplyr` and `lubridate` capability: Say we want the growth in the stock over the past year. We can do this with `dplyr` operations. 

Getting the last year is simple with `dplyr` and `lubridate`. We first `select` the date and adjusted price (adjusted for stock splits). We then `filter` using `lubridate` date functions.

```{r}
aapl_prices %>%
    select(date, adjusted) %>%
    filter(date >= today() - years(1))
```

We can also get a baseline price using the `first` function. Adding to our workflow, this looks like:

```{r}
aapl_prices %>%
    select(date, adjusted) %>%
    filter(date >= today() - years(1)) %>%
    mutate(baseline = first(adjusted))
```

Growth and growth percent versus baseline columns can be added now. We tack on a final `select` statement to remove unnecessary columns. The final workflow looks like this:

```{r}
aapl_growth <- aapl_prices %>%
    select(date, adjusted) %>%
    filter(date >= today() - years(1)) %>%
    mutate(baseline = first(adjusted),
           growth = adjusted - baseline,
           growth.pct = growth / baseline) %>%
    select(-(baseline:growth))
aapl_growth
```

And, we can quickly plot using `ggplot2`.

```{r}
aapl_growth %>%
    ggplot(aes(x = date, y = growth.pct)) + 
    geom_line() +
    labs(title = "AAPL: Growth Over One Year", x = "", y = "Growth") +
    scale_y_continuous(labels = scales::percent)
```



<a class="anchor" id="quant-power"></a>

## Leverage the Quantitative Power of xts, quantmod and TTR

You may already know and love `xts` / `zoo`, `quantmod` and `TTR`, which is why the core functionality is fully intact. Using `tq_transform()` and `tq_mutate()`, we can apply the `zoo`, `xts`, `quantmod` and `TTR` functions. Entering `tq_transform_fun_options()` returns a list the transform functions by each package. We'll discuss these options by package briefly.

```{r}
tq_transform_fun_options() %>% str()
```

### zoo Functionality

```{r}
# Get zoo functions that work with tq_transform and tq_mutate
tq_transform_fun_options()$zoo
```

The `zoo` functions that are compatible are listed above. Generally speaking, these are the:

* Roll Apply Functions:
    * A generic function for applying a function to rolling margins.
    * Form: `rollapply(data, width, FUN, ..., by = 1, by.column = TRUE, fill = if (na.pad) NA, na.pad = FALSE, partial = FALSE, align = c("center", "left", "right"), coredata = TRUE)`.
    * Options include `rollmax`, `rollmean`, `rollmedian`, `rollsum`, etc.


### xts Functionality


```{r}
# Get xts functions that work with tq_transform and tq_mutate
tq_transform_fun_options()$xts
```

The `xts` functions that are compatible are listed above. Generally speaking, these are the:

* Period Apply Functions:
    * Apply a function to a time segment (e.g. `max`, `min`, `mean`, etc).
    * Form: `apply.daily(x, FUN, ...)`.
    * Options include apply.daily, weekly, monthly, quarterly, yearly.

* To-Period Functions:
    * Convert a time series to time series of lower periodicity (e.g. convert daily to monthly periodicity).
    * Form: `to.period(x, period = 'months', k = 1, indexAt, name = NULL, OHLC = TRUE, ...)`.
    * Options include to.minutes, hourly, daily, weekly, monthly, quarterly, yearly.
    * __Note 1 (Important)__: The return structure is different for `to.period` and the `to.monthly` (`to.weekly`, `to.quarterly`, etc) forms. `to.period` returns a date, while `to.months` returns a character MON YYYY. Best to use `to.period` if you want to work with time-series via `lubridate`.  
     


### quantmod Functionality

```{r}
# Get quantmod functions that work with tq_transform and tq_mutate
tq_transform_fun_options()$quantmod
```

The `quantmod` functions that are compatible are listed above. Generally speaking, these are the:

* Percentage Change (Delt) and Lag Functions
    * Delt: `Delt(x1, x2 = NULL, k = 0, type = c("arithmetic", "log"))`
        * Variations of Delt: ClCl, HiCl, LoCl, LoHi, OpCl, OpHi, OpLo, OpOp 
        * Form: `OpCl(OHLC)`
    * Lag: `Lag(x, k = 1)` / Next: `Next(x, k = 1)` (Can also use `dplyr::lag` and `dplyr::lead`)
    

* Period Return Functions: 
    * Get the arithmetic or logarithmic returns for various periodicities, which include daily, weekly, monthly, quarterly, and yearly.
    * Form: `periodReturn(x, period = 'monthly', subset = NULL, type = 'arithmetic', leading = TRUE, ...)`

* Series Functions: 
    * Return values that describe the series. Options include describing the increases/decreases, accelerations/decelerations, and hi/low.
    * Forms: `seriesHi(x)`, `seriesIncr(x, thresh = 0, diff. = 1L)`, `seriesAccel(x)`

### TTR Functionality

```{r}
# Get TTR functions that work with tq_transform and tq_mutate
tq_transform_fun_options()$TTR
```


Here' a brief description of the most popular functions from `TTR`:

* Welles Wilder's Directional Movement Index: 
    *  `ADX(HLC, n = 14, maType, ...)`
* Bollinger Bands: 
    *  `BBands(HLC, n = 20, maType, sd = 2, ...)`: Bollinger Bands
* Rate of Change / Momentum: 
    * `ROC(x, n = 1, type = c("continuous", "discrete"), na.pad = TRUE)`: Rate of Change
    * `momentum(x, n = 1, na.pad = TRUE)`: Momentum
* Moving Averages (maType):
    * `SMA(x, n = 10, ...)`: Simple Moving Average
    * `EMA(x, n = 10, wilder = FALSE, ratio = NULL, ...)`: Exponential Moving Average
    * `DEMA(x, n = 10, v = 1, wilder = FALSE, ratio = NULL)`: Double Exponential Moving Average
    * `WMA(x, n = 10, wts = 1:n, ...)`: Weighted Moving Average
    * `EVWMA(price, volume, n = 10, ...)`: Elastic, Volume-Weighted Moving Average
    * `ZLEMA(x, n = 10, ratio = NULL, ...)`: Zero Lag Exponential Moving Average
    * `VWAP(price, volume, n = 10, ...)`: Volume-Weighted Moving Average Price
    * `VMA(x, w, ratio = 1, ...)`: Variable-Length Moving Average
    * `HMA(x, n = 20, ...)`: Hull Moving Average
    * `ALMA(x, n = 9, offset = 0.85, sigma = 6, ...)`: Arnaud Legoux Moving Average
* MACD Oscillator: 
    *  `MACD(x, nFast = 12, nSlow = 26, nSig = 9, maType, percent = TRUE, ...)`
* Relative Strength Index: 
    *  `RSI(price, n = 14, maType, ...)`
* runFun: 
    * `runSum(x, n = 10, cumulative = FALSE)`: returns sums over a n-period moving window.
    * `runMin(x, n = 10, cumulative = FALSE)`: returns minimums over a n-period moving window.
    * `runMax(x, n = 10, cumulative = FALSE)`: returns maximums over a n-period moving window.
    * `runMean(x, n = 10, cumulative = FALSE)`: returns means over a n-period moving window.
    * `runMedian(x, n = 10, non.unique = "mean", cumulative = FALSE)`: returns medians over a n-period moving window.
    * `runCov(x, y, n = 10, use = "all.obs", sample = TRUE, cumulative = FALSE)`: returns covariances over a n-period moving window.
    * `runCor(x, y, n = 10, use = "all.obs", sample = TRUE, cumulative = FALSE)`: returns correlations over a n-period moving window.
    * `runVar(x, y = NULL, n = 10, sample = TRUE, cumulative = FALSE)`: returns variances over a n-period moving window.
    * `runSD(x, n = 10, sample = TRUE, cumulative = FALSE)`: returns standard deviations over a n-period moving window.
    * `runMAD(x, n = 10, center = NULL, stat = "median", constant = 1.4826, non.unique = "mean", cumulative = FALSE)`: returns median/mean absolute deviations over a n-period moving window.
    * `wilderSum(x, n = 10)`: retuns a Welles Wilder style weighted sum over a n-period moving window.
* Stochastic Oscillator / Stochastic Momentum Index:
    * `stoch(HLC, nFastK = 14, nFastD = 3, nSlowD = 3, maType, bounded = TRUE, smooth = 1, ...)`: Stochastic Oscillator
    * `SMI(HLC, n = 13, nFast = 2, nSlow = 25, nSig = 9, maType, bounded = TRUE, ...)`: Stochastic Momentum Index


### Quantitative Power In Action

We'll go through some examples, but first let's get some data. The default for `tq_get()` is `get = "stock.prices"`, so all we need is to give `x` a stock symbol.

```{r}
AAPL <- tq_get("AAPL")
```


#### Example 1: Getting the Max Close Price for Each Quarter.

The `xts::apply.quarterly()` function that is part of the period apply group can be used to apply functions by quarterly time segments. Because we are seeking a return structure that is on a different time scale than the input (quarterly versus daily), we need to use a transform function. We select `tq_transform` and pass the close price using OHLC format via `ohlc_fun = Cl`, and we send this subset of the data to the `apply.quarterly` function via the `transform_fun` argument. Looking at the documentation for `apply.quarterly`, we see that we can pass a function to the argument, `FUN`. We want the maximum values, so we set `FUN = max`. The result is the quarters returned as a date and the maximum closing price during the quarter returned as a double. 

```{r}
AAPL %>%
    tq_transform(ohlc_fun = Cl, transform_fun = apply.quarterly, FUN = max)
```

Note that as an alternative you could use the xy form, replacing `ohlc_fun = Cl` with `x = close`.

#### Example 2: Getting Daily Log Returns 

The `quantmod::periodReturn()` function generates returns by periodicity. We have a few options here. Normally I go with a transform function, `tq_transform`, because the `periodReturn` function accepts different periodicity options, and anything other than daily will blow up a mutation. But, in our situation the period returns periodicity is the same as the stock prices periodicity (both daily), so we can use either. We want to use the adjusted closing prices column (adjusted for stock splits, which can make it appear that a stock is performing poorly if a split is included), so we set `ohlc_fun = Ad`. We researched the `periodReturn` function, and we found that it accepts `type = "log"` and `period = "daily"`, which returns the daily log returns. 


```{r}
AAPL %>%
    tq_transform(ohlc_fun = Ad, transform_fun = periodReturn, 
                 type = "log", period = "daily")
```

#### Example 3: Adding MACD and Bollinger Bands to a OHLC data set

In reviewing the available options in the `TTR` package, we see that `MACD` and `BBands` functions will get us where we need to be. In researching the documentation, the return is in the same periodicity as the input and the functions work with OHLC functions, so we can use `tq_mutate()`. MACD requires a price, so we select close using `Cl`, BBands requires high, low, and close, prices so we use `HLC`. We can chain the inputs together using the pipe (`%>%`) since mutate just adds columns. The result is a tibble containing the MACD and Bollinger Band results. 

```{r}
AAPL %>%
    tq_mutate(Cl, MACD) %>%
    tq_mutate(HLC, BBands)
```

Note that for the MACD, we could have used `tq_mutate_xy()`, setting `x = close`. However, for the BBands, we are forced to use `tq_mutate()` because of the HLC input.

#### Example 4: Getting the Percentage Difference Between Open and Close from Zero to Five Periods 

We can't use the `OpCl` function for this task since it only returns the percentage difference for a period lag of zero. We keep digging and we find the base `Delt` function from quantmod. In researching the function, we see that `Delt` takes one or two inputs, `k` a series of lags, and the type of difference, either arithmetic or log. We will set `x = open` and `y = close` and `k = 0:5` to get zero through five periods. The default `type = "arithmetic"` is acceptable, so there is no need to specify. The result is the percentage difference between the open and close prices for periods zero to five.

```{r}
AAPL %>%
    tq_mutate_xy(x = open, y = close, mutate_fun = Delt, k = 0:5) %>%
    select(-c(high, low, volume, adjusted))
```

For comparison we'll inspect the output from the `OpCl()` function using `tq_mutate()`. We send OHLC prices to the OpCl function. As expected the `OpCl..` column returned is the same as `Delt.0.arithmetic` from above.

```{r}
AAPL %>%
    tq_mutate(OHLC, OpCl) %>%
    select(-c(high, low, volume, adjusted))
```

#### Example 4: Get the 5, 10, 15-Day Rolling Minimum and Maximum Values of the Adjusted Prices

Rolling functions come from the `zoo` package. In reviewing the available options, we see that `rollmax` is present but no there is no `rollmin`. However, the generic `rollapply` function will work with any function, so let's use that. In reviewing the documentation, the `rollapply()` function takes three primary arguments: `data`, `width` and `FUN`. The `data` is passed using the `ohlc_fun` argument, so we don't need to worry about this. The `width` is the number of periods to apply the function, and for our situation this is 5, 10, and 15. The `FUN` is the function to apply, and for our situation this is `min` and `max`. We want the result to add columns to our data set, so we use the `tq_mutate()` function, which allows us to pipe (`%>%`) each mutation. Putting it all together:

```{r}
AAPL %>%
    tq_mutate(Ad, rollapply, width = 5, FUN = min) %>%
    tq_mutate(Ad, rollapply, width = 10, FUN = min) %>%
    tq_mutate(Ad, rollapply, width = 15, FUN = min) %>%
    tq_mutate(Ad, rollapply, width = 5, FUN = max) %>%
    tq_mutate(Ad, rollapply, width = 10, FUN = max) %>%
    tq_mutate(Ad, rollapply, width = 15, FUN = max)
```

Note that the new column names for the rolling minimums and maximums are just the `mutate_fun` name with a sequential suffix added, which is likely not descriptive enough for our applications. These can be renamed using the `rename` function (e.g. `rename(rolling.min.5 = rollapply)` for the five period rolling minimum). 

<a class="anchor" id="built-for-scale"></a>

## Designed to be Used and Scaled with the tidyverse

Each function has one primary input and one output. This allows chaining operations with the pipe (`%>%`), and mapping to extend to lists of many stocks, exchange rates, metals, economic data, financial statements, etc. The rationale behind this is simple: let the function handle the operation, let the `tidyverse` handle the iteration. 

Rather than explain, let's go through a simple workflow using the `tidyverse`. We setup a two step workflow:

1. Analyze a single stock
2. Scale to many stocks

#### Analyze a Single Stock

In our hypothetical situation, we want to compare the mean monthly log returns (MMLR). First, let's come up with a function to help us collect log returns. The function below performs three operations internally. It first gets the stock prices using `tq_get()`. Then, it transforms the stock prices to period returns using `tq_transform()`. We add the `type = "log"` and `period = "monthly"` arguments to ensure we retrieve a tibble of monthly log returns. Last, we take the mean of the monthly returns to get MMLR.

```{r}
my_stock_analysis_fun <- function(stock.symbol) {
    period.returns <- stock.symbol %>%
        tq_get(get = "stock.prices") %>%
        tq_transform(ohlc_fun = Ad, transform_fun = periodReturn, 
                     type = "log", period = "monthly")
    mean(period.returns$monthly.returns)
}
```

And, let's test it out. We now have the mean monthly log returns over the past ten years.

```{r}
my_stock_analysis_fun("AAPL")
```



### Extrapolate to Many Stocks using `tidyverse`

Now that we have one stock down, we can scale to many stocks. For brevity, we'll randomly sample ten stocks from the S&amp;P500 with a call to `dplyr::sample_n()`.

```{r}
set.seed(100)
stocks <- tq_get("SP500", get = "stock.index") %>%
    sample_n(10)
stocks
```

We can now apply our analysis function to the stocks using `dplyr::mutate` and `purrr::map_dbl`. The `mutate()` function adds a column to our tibble, and the `map_dbl()` function maps our `my_stock_analysis_fun` to our tibble of stocks using the `symbol` column.

```{r}
stocks <- stocks %>%
    mutate(mmlr = map_dbl(symbol, my_stock_analysis_fun)) %>%
    arrange(desc(mmlr))
stocks
```

And, we're done! We now have the MMLR for 10-years of stock data for 10 stocks. And, we can easily extend this to larger lists or stock indexes. For example, the entire S&amp;P500 could be analyzed removing the `sample_n()` following the call to `tq_get("SP500", get = "stock.index")`.

### Function `tq_get()` Designed to Handle Errors Gracefully

Eventually you will run into a stock index, stock symbol, FRED data code, etc that cannot be retrieved. Possible reasons are: 

* The website changes
* An index becomes out of date
* A company goes private
* A stock ticker symbol changes
* Yahoo / FRED just doesn't like your stock symbol / FRED code

This becomes painful when scaling if the functions return errors. So, the `tq_get()` function is designed to handle errors gracefully. What this means is a `NA` value is returned when an error is generated along with a gentle error warning. There are pros and cons to this approach that you may not agree with but I believe helps in the long run. Just be aware of what happens:

* __Pros__: Long running scripts are not interrupted because of one error

* __Cons__: Errors flow downstream if not looking at warnings and not reviewing results


__With `tq_get()`, Bad Apples Fail Gracefully__:

Let's see an example when mapping to `tq_get()` to a long list of stocks with one `BAD APPLE`.

```{r, warning = TRUE}
stock_list_with_one_bad_apple <- tibble( 
    symbol = c("AAPL", "GOOG", "AMZN", "FB", "BAD APPLE",
               "AVGO", "SWKS","NVDA", "V", "MA")
)
stock_list_with_one_bad_apple <- stock_list_with_one_bad_apple %>%
    mutate(stock.prices = map(.x = symbol, ~ tq_get(x = .x, get = "stock.prices")))
```

We get warned that there was an issue in the operation. With that said, we still get the full list of stocks.

```{r}
stock_list_with_one_bad_apple
```


Say hypothetically we didn't recognize the error message. An error shows up during the next operation. As an example, we'll attempt to get yearly period returns using `tq_transform`. The operation is wrapped in a `tryCatch()` statement to enable printing the error message. 

```{r}
tryCatch({
    stock_list_with_one_bad_apple %>%
    mutate(annual.returns = map(.x = stock.prices, 
                                ~ tq_transform(x = .x,
                                               ohlc_fun = Ad, 
                                               transform_fun = periodReturn, 
                                               period = "yearly")
                                )
           )
}, error = function(e) {
    print(e)
})


```

The operation grinds to a hault because the `BAD APPLE` tried to send its value for stock.prices of `NA` to the `tq_transform()` function. The error message tells us that `data` is not a `tibble` or `data.frame`.

The rationale behind the error handling approach is that long-running scripts should not fail during minor issues. For example, if you have a list of 3000 stocks and the 3000th is bad, the program could take 20+ minutes to fail. This is disheartening. We allow `tq_get()` to continue to fetch data even if an error is encountered. Failure occurs during `tq_transform()` and `tq_mutate()` to prevent the error from getting too far downstream. 

Recognizing how `tq_get()` works (and gracefully fails), we can adjust our workflow. It's a good idea to collect stock information in one independent step, review any warnings / errors, and remove "bad apples" if present before moving on to any transformations or mutations.

Here's an example of a good workflow:

```{r}
stock_list_with_one_bad_apple <- tibble( 
    symbol = c("AAPL", "GOOG", "AMZN", "FB", "BAD APPLE",
               "AVGO", "SWKS","NVDA", "V", "MA")
    ) %>%
    # Step 1: Get stock prices
    mutate(stock.prices = map(.x = symbol, ~ tq_get(x = .x, get = "stock.prices")),
           class = map_chr(.x = stock.prices, ~ class(.x)[[1]])) %>%
    # Step 2: Filter out errors; errors have a class of "logical"
    filter(class != "logical") %>%
    select(-class) %>%
    # Step 3: Perform period returns
    mutate(annual.returns = map(.x = stock.prices, 
                                ~ tq_transform(data = .x,
                                               ohlc_fun = Ad, 
                                               transform_fun = periodReturn, 
                                               period = "yearly")
                                )
           )
stock_list_with_one_bad_apple
```


__Fall Back for Stock Indexes__:

There's a fallback for the stock indexes too. Since the source, www.marketvolume.com, could change over time, an option is provided to pull stored data within the `tidyquant` package. The downside is that the data is only as accurate as the last update to `tidyquant`. Here's how to get the stock indexes locally if for some reason the website is down or has changed.

```{r, warning = TRUE, message = TRUE}
tq_get("SP500", get = "stock.index", use_fallback = TRUE)
```


# Recap

Hopefully now you see how `tidyquant` helps to integrate the best quantitative financial analysis packages with the `tidyverse`. With a few, easy-to-use core functions, you can efficiently leverage the quantitative power of `xts` and `zoo`, `quantmod` and `TTR` with the data management infrastructure and scale-ability of the `tidyverse`. 

